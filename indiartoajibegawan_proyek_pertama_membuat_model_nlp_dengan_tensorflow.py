# -*- coding: utf-8 -*-
"""IndiartoAjiBegawan_Proyek Pertama : Membuat Model NLP dengan TensorFlow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0iDb_PZWqaNbD-BRk9f-_8F12fyFxbV

# Identitas Diri
Nama : Indiarto Aji Begawan \
Email : indiartoaji13@gmail.com \
Learning Path : Belajar Pengembangan Machine Learning\
Materi : Proyek Pertama - Membuat Model NLP dengan TensorFlow

# Library
"""

import re
import nltk
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout, LSTM

"""# Cleaning Text (Function)
(https://stackoverflow.com/questions/54733828/remove-twitter-mentions-from-pandas-column)
"""

def clean_text(row, options):

    if options['lowercase']:
        row = row.lower()

    if options['remove_url']:
        row = row.replace('http\S+|www.\S+|.com\S+', '')

    if options['remove_mentions']:
        row = re.sub("@[A-Za-z0-9_]+","", row)

    if options['remove_hastags']:
        row = re.sub("#+","", row)

    return row

clean_config = {
    'remove_url': True,
    'remove_mentions': True,
    'lowercase': True,
    'remove_hastags' : True
    }

nltk.download('stopwords')
stop = stopwords.words('english')

"""# Corona Dataset"""

df = pd.read_csv('Corona_NLP_test.csv').dropna()
df

"""Dataset merupakan data publik yang diambil dari situs Kaggle dengfan nama Coronavirus tweets NLP - Text Classification (https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification) yang berisikan 3798 data dan 6 kolom."""

df = df.drop(columns=['UserName', 'ScreenName','Location', 'TweetAt'])
df

df.info()

"""Kolom yang diambil hanyalah kolom OriginalTweet dan Sentiment. Kolom OriginalTweet akan menjadi data masukan dan kolom Sentiment akan menjadi label."""

df.loc[df.Sentiment == "Extremely Negative","Sentiment"] = "Negative"
df.loc[df.Sentiment == "Extremely Positive","Sentiment"] = "Positive"
df

"""Terdapat 5 label yaitu Extremely Negative, negative, Neutral, Positive, dan Extremely Positive. Label Extremely Negative dilebur menjadi Negative dan Extremely Positive dilebur menjadi Positive. Hasilnya hanya ada 3 label yaitu negative, Neutral, dan Positive."""

category = pd.get_dummies(df.Sentiment)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Sentiment')
df_baru

"""Label diubah menjadi data biner untuk memudahkan dalam identifikasi oleh machine."""

df_baru['tweet'] = df_baru['OriginalTweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df_baru['tweet'] = df_baru['tweet'].apply(clean_text, args=(clean_config,))
# df_baru["tweet"] = df_baru['tweet'].str.replace('[^\w\s]','')
df_baru

"""Data masukan diproses untuk menghilangkan stop word(English). Data masukan diproses menggunakan fungsi clean_data untuk menghilangkan URL, Mention, Hastags, dan mengubah kalimat menjadi lower case."""

tweet = df_baru['tweet'].values
label = df_baru[['Negative', 'Neutral', 'Positive']].values
num_classes = len(label[0])

"""Data dalam dataframe diubah menjadi bentuk array dalam variabel tweet(data masukan) dan label."""

x_train, x_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2)
print(len(x_train))
print(len(y_train))
print(len(x_test))
print(len(y_test))

"""Data tersebut kemudian displit menjadi data latih dan data tes."""

max_kata = 60

token = Tokenizer(num_words=15000,oov_token='-')
token.fit_on_texts(x_train)
token.fit_on_texts(x_test)

x_train = token.texts_to_sequences(x_train)
x_test = token.texts_to_sequences(x_test)

x_train = pad_sequences(x_train, maxlen = max_kata, padding="post") 
x_test = pad_sequences(x_test, maxlen = max_kata, padding="post")

"""Masing-masing kalimat diubah menjadi angka dengan menggunakan fungsi Tokenizer."""

token.index_word
vocab = len(token.index_word)+1
vocab

"""Mengukur banyak kata yang ada pada data masukan."""

vec_size=300
model = Sequential()
model.add(Embedding(vocab, vec_size, input_length = max_kata))
model.add(LSTM(128))
model.add(Dropout(0.5))
# model.add(Dense(32, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])

"""Membangun model untuk proses training menggunakan metode LSTM."""

hist = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), verbose=2)

"""Proses training sebanyak 30 epochs tetapi menghasilkan model yang overfitting karena nilai akurasi pada train data lebih besar dari akurasi pada validation data."""

# Grafik Accuracy dan Validation Accuracy
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Grafik Accuracy dan Validation Accuracy
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""Terlihat pada grafik bahwa nilai akurasi pada test data berada jauh dibawah train data. Terdapat beberapa hipotesa mengenai hal tersebut salah satunya adalah jumlah data yang sangat sedikit yaitu sekitar 2000an. Untuk memperbaiki hal tersebut, saya telah mencoba beberapa parameter seperti merubah vec_size dan nilai LSTM tetapi tidak memberikan improvement yang signifinak. Nilai LSTM ini dipilih dengan mempertimbangkan waktu training, nilai akurasi (train dan val data), dan improvement pada tiap epoch.

# Emotion Dataset
"""

df1 = pd.read_csv('emotion-labels-train.csv').dropna()
df2 = pd.read_csv('emotion-labels-test.csv').dropna()
df3 = pd.read_csv('emotion-labels-val.csv').dropna()
df = pd.concat([df1, df2, df3], axis=0)
df

"""Untuk memperbaiki permasalahan pada dataset sebelumnya maka saya mencari dataset baru pada Kaggle dengan nama Emotion Classification NLP (https://www.kaggle.com/datasets/anjaneyatripathi/emotion-classification-nlp?select=emotion-labels-val.csv).
Data yang diperoleh telah dibagi menjadi data train, test, dan data validation. Disini saya menggabungkan data tersebut menjadi dataset yang utuh dan kemudian menggunakan fungsi train_test_split untuk membagi data menjadi data train dan test. Hal tersebut dilakukan untuk menanggulangi hipotesis pada dataset sebelumnya yaitu dataset yang sedikit. Jumlah data pada dataset ini adalah 7102 data setelah menghilangkan nilai Null menggunakan lib pandas.
"""

label = pd.get_dummies(df.label)
df_baru = pd.concat([df, label], axis=1)
df_baru = df_baru.drop(columns='label')
df_baru

"""Proses diatas bertujuan untuk mengubah label yang berupa string menjadi nilai biner yang dapat diproses oleh mesin."""

df_baru.info()

"""df.info() berguna untuk menampilkan detail mengenaik dataframe yang akan di proses."""

df_baru['tweet'] = df_baru['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df_baru['tweet'] = df_baru['tweet'].apply(clean_text, args=(clean_config,))
df_baru["tweet"] = df_baru['tweet'].str.replace('[^\w\s]','')
df_baru

"""Seperti yang telah dijelaskan pada dataset sebelumnya, function clean_text berguna untuk menghilangkan mention, website, hastags, dan dibuat lower case."""

tweet = df_baru['tweet'].values
label = df_baru[['anger', 'fear', 'joy', 'sadness']].values
num_classes = len(label[0])

"""Block diatas bertujuan untuk membagi data masukan dan label."""

x_train, x_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2)
print(len(x_train))
print(len(y_train))
print(len(x_test))
print(len(y_test))

"""function diatas digunakan untuk membagi data train dan test dengan rasio 8:2."""

max_kata = 30

token = Tokenizer(num_words=15000,oov_token='-')
token.fit_on_texts(x_train)
token.fit_on_texts(x_test)

x_train = token.texts_to_sequences(x_train)
x_test = token.texts_to_sequences(x_test)

x_train = pad_sequences(x_train, maxlen = max_kata, padding="post") 
x_test = pad_sequences(x_test, maxlen = max_kata, padding="post")

"""block diatas bertujuan untuk mengidentifikasi kata menjadi urutan angka untuk mempermudah mesin untuk proses identifikasi."""

token.index_word
vocab = len(token.index_word)+1
vocab

"""terdapat 13344 vocabulary pada data tersebut."""

vec_size=300
model = Sequential()
model.add(Embedding(vocab, vec_size, input_length = max_kata))
model.add(LSTM(256))
model.add(Dropout(0.5))
# model.add(Dense(32, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(16, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])

"""Model yang dibuat sama seperti yang digunakan pada dataset sebelumnya."""

histo = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test), verbose=2)

"""Proses training dilakukan sebanyak 30 epoch dan menghasilkan nilai akurasi yang jauh lebih baik yaitu untuk data train dan test yaitu kurang lebih 97% dan 87%."""

# Grafik Accuracy dan Validation Accuracy
plt.plot(histo.history['accuracy'])
plt.plot(histo.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Grafik Accuracy dan Validation Accuracy
plt.plot(histo.history['loss'])
plt.plot(histo.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""Grafik diatas menunjukan Akurasi dan Loss. Nilai akurasi pada train dan test memiliki selisih yang cukup kecil dibanding pada dataset sebelumnya yaitu +- 10%.
Selain itu pada nilai loss, terdapat perbedaan yang sangat signifikan yaitu pada train loss cenderung kovergen dan semakin mendekati nilai nol sedangkan test loss mengalami kebalikannya.
"""